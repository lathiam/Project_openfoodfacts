{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pXBrgilbjjnF"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SYlrXIcsxiZz",
        "outputId": "7fbb3a7c-d4ea-448b-f806-a8e991c9f0a8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (2.2.2)\n",
            "Requirement already satisfied: google-cloud-storage in /usr/local/lib/python3.11/dist-packages (2.19.0)\n",
            "Requirement already satisfied: numpy>=1.23.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (1.26.4)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.1)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.1)\n",
            "Requirement already satisfied: google-auth<3.0dev,>=2.26.1 in /usr/local/lib/python3.11/dist-packages (from google-cloud-storage) (2.38.0)\n",
            "Requirement already satisfied: google-api-core<3.0.0dev,>=2.15.0 in /usr/local/lib/python3.11/dist-packages (from google-cloud-storage) (2.24.1)\n",
            "Requirement already satisfied: google-cloud-core<3.0dev,>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from google-cloud-storage) (2.4.2)\n",
            "Requirement already satisfied: google-resumable-media>=2.7.2 in /usr/local/lib/python3.11/dist-packages (from google-cloud-storage) (2.7.2)\n",
            "Requirement already satisfied: requests<3.0.0dev,>=2.18.0 in /usr/local/lib/python3.11/dist-packages (from google-cloud-storage) (2.32.3)\n",
            "Requirement already satisfied: google-crc32c<2.0dev,>=1.0 in /usr/local/lib/python3.11/dist-packages (from google-cloud-storage) (1.6.0)\n",
            "Requirement already satisfied: googleapis-common-protos<2.0.dev0,>=1.56.2 in /usr/local/lib/python3.11/dist-packages (from google-api-core<3.0.0dev,>=2.15.0->google-cloud-storage) (1.69.0)\n",
            "Requirement already satisfied: protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0.dev0,>=3.19.5 in /usr/local/lib/python3.11/dist-packages (from google-api-core<3.0.0dev,>=2.15.0->google-cloud-storage) (4.25.6)\n",
            "Requirement already satisfied: proto-plus<2.0.0dev,>=1.22.3 in /usr/local/lib/python3.11/dist-packages (from google-api-core<3.0.0dev,>=2.15.0->google-cloud-storage) (1.26.0)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from google-auth<3.0dev,>=2.26.1->google-cloud-storage) (5.5.2)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.11/dist-packages (from google-auth<3.0dev,>=2.26.1->google-cloud-storage) (0.4.1)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.11/dist-packages (from google-auth<3.0dev,>=2.26.1->google-cloud-storage) (4.9)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0dev,>=2.18.0->google-cloud-storage) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0dev,>=2.18.0->google-cloud-storage) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0dev,>=2.18.0->google-cloud-storage) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0dev,>=2.18.0->google-cloud-storage) (2025.1.31)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.4.6 in /usr/local/lib/python3.11/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3.0dev,>=2.26.1->google-cloud-storage) (0.6.1)\n"
          ]
        }
      ],
      "source": [
        "pip install pandas google-cloud-storage"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "em__SM3ojfws"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HWH-MF61joNb",
        "outputId": "883c7953-1456-4f1b-9267-3b67441bfbad"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " Connexion √† l'API OpenFoodFacts √©tablie.\n",
            " T√©l√©chargement page 1/20\n",
            " T√©l√©chargement page 2/20\n",
            " T√©l√©chargement page 3/20\n",
            " T√©l√©chargement page 4/20\n",
            " T√©l√©chargement page 5/20\n",
            " T√©l√©chargement page 6/20\n",
            " T√©l√©chargement page 7/20\n",
            " T√©l√©chargement page 8/20\n",
            " T√©l√©chargement page 9/20\n",
            " T√©l√©chargement page 10/20\n",
            " T√©l√©chargement page 11/20\n",
            "üî¥ Erreur HTTP page 11 : 429 Client Error: Too Many Requests for url: https://world.openfoodfacts.org/cgi/search.pl?action=process&page_size=1000&page=11&json=True\n",
            "‚ö†Ô∏è Page 11 vide ou invalide. Passage √† la suivante.\n",
            " T√©l√©chargement page 12/20\n",
            "üî¥ Erreur HTTP page 12 : 429 Client Error: Too Many Requests for url: https://world.openfoodfacts.org/cgi/search.pl?action=process&page_size=1000&page=12&json=True\n",
            "‚ö†Ô∏è Page 12 vide ou invalide. Passage √† la suivante.\n",
            " T√©l√©chargement page 13/20\n",
            "üî¥ Erreur HTTP page 13 : 429 Client Error: Too Many Requests for url: https://world.openfoodfacts.org/cgi/search.pl?action=process&page_size=1000&page=13&json=True\n",
            "‚ö†Ô∏è Page 13 vide ou invalide. Passage √† la suivante.\n",
            " T√©l√©chargement page 14/20\n",
            "üî¥ Erreur HTTP page 14 : 429 Client Error: Too Many Requests for url: https://world.openfoodfacts.org/cgi/search.pl?action=process&page_size=1000&page=14&json=True\n",
            "‚ö†Ô∏è Page 14 vide ou invalide. Passage √† la suivante.\n",
            " T√©l√©chargement page 15/20\n",
            "üî¥ Erreur HTTP page 15 : 429 Client Error: Too Many Requests for url: https://world.openfoodfacts.org/cgi/search.pl?action=process&page_size=1000&page=15&json=True\n",
            "‚ö†Ô∏è Page 15 vide ou invalide. Passage √† la suivante.\n",
            " T√©l√©chargement page 16/20\n",
            "üî¥ Erreur HTTP page 16 : 429 Client Error: Too Many Requests for url: https://world.openfoodfacts.org/cgi/search.pl?action=process&page_size=1000&page=16&json=True\n",
            "‚ö†Ô∏è Page 16 vide ou invalide. Passage √† la suivante.\n",
            " T√©l√©chargement page 17/20\n",
            "üî¥ Erreur HTTP page 17 : 429 Client Error: Too Many Requests for url: https://world.openfoodfacts.org/cgi/search.pl?action=process&page_size=1000&page=17&json=True\n",
            "‚ö†Ô∏è Page 17 vide ou invalide. Passage √† la suivante.\n",
            " T√©l√©chargement page 18/20\n",
            "üî¥ Erreur HTTP page 18 : 429 Client Error: Too Many Requests for url: https://world.openfoodfacts.org/cgi/search.pl?action=process&page_size=1000&page=18&json=True\n",
            "‚ö†Ô∏è Page 18 vide ou invalide. Passage √† la suivante.\n",
            " T√©l√©chargement page 19/20\n",
            "üî¥ Erreur HTTP page 19 : 429 Client Error: Too Many Requests for url: https://world.openfoodfacts.org/cgi/search.pl?action=process&page_size=1000&page=19&json=True\n",
            "‚ö†Ô∏è Page 19 vide ou invalide. Passage √† la suivante.\n",
            " T√©l√©chargement page 20/20\n",
            "üî¥ Erreur HTTP page 20 : 429 Client Error: Too Many Requests for url: https://world.openfoodfacts.org/cgi/search.pl?action=process&page_size=1000&page=20&json=True\n",
            "‚ö†Ô∏è Page 20 vide ou invalide. Passage √† la suivante.\n",
            "‚úÖ 1000 produits extraits.\n",
            "‚úÖ Fichier CSV sauvegard√© : /content/openfood_referentiel.csv\n",
            "‚úÖ Donn√©es charg√©es dans BigQuery : project-final-laka-93110.Laka10.openfoodfacts\n",
            " 1000 lignes r√©cup√©r√©es depuis BigQuery\n",
            " Transformation des donn√©es...\n",
            " Donn√©es transform√©es.\n",
            "‚úÖ Donn√©es transform√©es sauvegard√©es.\n"
          ]
        }
      ],
      "source": [
        "import requests\n",
        "import pandas as pd\n",
        "import time\n",
        "import os\n",
        "from google.cloud import bigquery\n",
        "\n",
        "#  Authentification GCP\n",
        "os.environ[\"GOOGLE_APPLICATION_CREDENTIALS\"] = \"/content/project-final-laka-93110-e98c1369c8cc.json\"\n",
        "\n",
        "#Extraction globale\n",
        "PAGE_SIZE = 1000\n",
        "NUM_PAGES = 20\n",
        "CSV_PATH = \"/content/openfood_referentiel.csv\"\n",
        "\n",
        "# BigQuery\n",
        "PROJECT_ID = \"project-final-laka-93110\"\n",
        "DATASET_ID = \"Laka10\"\n",
        "TABLE_ID = \"openfoodfacts\"\n",
        "BQ_TABLE = f\"{PROJECT_ID}.{DATASET_ID}.{TABLE_ID}\"\n",
        "\n",
        "#  V√©rification de connexion √† l'API\n",
        "def check_api_connection():\n",
        "    try:\n",
        "        response = requests.get(\"https://world.openfoodfacts.org/\", timeout=5)\n",
        "        if response.status_code == 200:\n",
        "            print(\" Connexion √† l'API OpenFoodFacts √©tablie.\")\n",
        "            return True\n",
        "    except Exception as e:\n",
        "        print(f\" √âchec de connexion √† l'API : {e}\")\n",
        "    return False\n",
        "\n",
        "# Fonction : r√©cup√©ration s√©curis√©e des produits\n",
        "def fetch_all_products(page, page_size=1000):\n",
        "    url = \"https://world.openfoodfacts.org/cgi/search.pl\"\n",
        "    params = {\n",
        "        \"action\": \"process\",\n",
        "        \"page_size\": page_size,\n",
        "        \"page\": page,\n",
        "        \"json\": True,\n",
        "    }\n",
        "    try:\n",
        "        res = requests.get(url, params=params, timeout=10)\n",
        "        res.raise_for_status()\n",
        "        return res.json().get(\"products\", [])\n",
        "    except requests.exceptions.RequestException as e:\n",
        "        print(f\"Erreur HTTP page {page} : {e}\")\n",
        "    except ValueError as e:\n",
        "        print(f\"Erreur JSON page {page} : {e}\")\n",
        "    return []\n",
        "\n",
        "# Extraction d'infos produit\n",
        "def extract_product_info(product):\n",
        "    return {\n",
        "        \"product_name\": product.get(\"product_name\", \"\"),\n",
        "        \"brands\": product.get(\"brands\", \"\"),\n",
        "        \"stores\": product.get(\"stores\", \"\"),\n",
        "        \"nutriscore_grade\": product.get(\"nutriscore_grade\", \"\"),\n",
        "        \"nutrition_score_fr\": product.get(\"nutrition_score_fr\", \"\"),\n",
        "        \"energy_kcal\": product.get(\"nutriments\", {}).get(\"energy-kcal_100g\"),\n",
        "        \"fat_100g\": product.get(\"nutriments\", {}).get(\"fat_100g\"),\n",
        "        \"saturated_fat_100g\": product.get(\"nutriments\", {}).get(\"saturated-fat_100g\"),\n",
        "        \"sugars_100g\": product.get(\"nutriments\", {}).get(\"sugars_100g\"),\n",
        "        \"salt_100g\": product.get(\"nutriments\", {}).get(\"salt_100g\"),\n",
        "        \"fiber_100g\": product.get(\"nutriments\", {}).get(\"fiber_100g\"),\n",
        "        \"proteins_100g\": product.get(\"nutriments\", {}).get(\"proteins_100g\"),\n",
        "        \"labels\": product.get(\"labels\", \"\"),\n",
        "        \"origins\": product.get(\"origins\", \"\"),\n",
        "        \"categories\": product.get(\"categories\", \"\"),\n",
        "        \"url\": product.get(\"url\", \"\"),\n",
        "        \"code\": product.get(\"code\", \"\"),\n",
        "    }\n",
        "\n",
        "def save_to_csv(df, path):\n",
        "    df.to_csv(path, index=False)\n",
        "    print(f\"Fichier CSV sauvegard√© : {path}\")\n",
        "\n",
        "def load_to_bigquery(csv_path, table_id):\n",
        "    client = bigquery.Client()\n",
        "    job_config = bigquery.LoadJobConfig(\n",
        "        source_format=bigquery.SourceFormat.CSV,\n",
        "        skip_leading_rows=1,\n",
        "        autodetect=True,\n",
        "        write_disposition=\"WRITE_TRUNCATE\"\n",
        "    )\n",
        "    with open(csv_path, \"rb\") as source_file:\n",
        "        job = client.load_table_from_file(source_file, table_id, job_config=job_config)\n",
        "    job.result()\n",
        "    print(f\"Donn√©es charg√©es dans BigQuery : {table_id}\")\n",
        "\n",
        "def get_data_from_bigquery(table_id):\n",
        "    client = bigquery.Client()\n",
        "    query = f\"SELECT * FROM `{table_id}`\"\n",
        "    df = client.query(query).to_dataframe()\n",
        "    print(f\" {len(df)} lignes r√©cup√©r√©es depuis BigQuery\")\n",
        "    return df\n",
        "\n",
        "def transform_data(df):\n",
        "    if df.empty:\n",
        "        print(\" DataFrame vide : aucune transformation possible.\")\n",
        "        return df\n",
        "\n",
        "    print(\" Transformation des donn√©es...\")\n",
        "\n",
        "    df['energy_kcal'] = pd.to_numeric(df['energy_kcal'], errors='coerce')\n",
        "    df['fat_100g'] = pd.to_numeric(df['fat_100g'], errors='coerce')\n",
        "    df['sugars_100g'] = pd.to_numeric(df['sugars_100g'], errors='coerce')\n",
        "    df['nutriscore_grade'] = df['nutriscore_grade'].str.upper()\n",
        "    df['product_name'] = df['product_name'].fillna(\"Inconnu\")\n",
        "    df['has_label_bio'] = df['labels'].str.contains(\"bio\", case=False, na=False)\n",
        "\n",
        "    df[\"nutrient_density\"] = df[\"energy_kcal\"] / (\n",
        "        df[\"proteins_100g\"].astype(float).fillna(0) +\n",
        "        df[\"fat_100g\"].astype(float).fillna(0) +\n",
        "        df[\"sugars_100g\"].astype(float).fillna(0)\n",
        "    ).replace(0, 1)\n",
        "\n",
        "    print(\" Donn√©es transform√©es.\")\n",
        "    return df\n",
        "\n",
        "#  Main pipeline\n",
        "def main():\n",
        "    if not check_api_connection():\n",
        "        print(\" Arr√™t du pipeline car l‚ÄôAPI OpenFoodFacts est injoignable.\")\n",
        "        return\n",
        "\n",
        "    all_products = []\n",
        "    for page in range(1, NUM_PAGES + 1):\n",
        "        print(f\" T√©l√©chargement page {page}/{NUM_PAGES}\")\n",
        "        products = fetch_all_products(page, PAGE_SIZE)\n",
        "        if not products:\n",
        "            print(f\"Page {page} vide ou invalide. Passage √† la suivante.\")\n",
        "            continue\n",
        "        all_products.extend([extract_product_info(p) for p in products])\n",
        "        time.sleep(1)\n",
        "\n",
        "    df = pd.DataFrame(all_products)\n",
        "    print(f\"{len(df)} produits extraits.\")\n",
        "\n",
        "    save_to_csv(df, CSV_PATH)\n",
        "    load_to_bigquery(CSV_PATH, BQ_TABLE)\n",
        "\n",
        "    bq_df = get_data_from_bigquery(BQ_TABLE)\n",
        "    transformed_df = transform_data(bq_df)\n",
        "    transformed_df.to_csv(\"/content/openfood_transformed.csv\", index=False)\n",
        "    print(\"Donn√©es transform√©es sauvegard√©es.\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zJvS4eOh31ED"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2zyejhkd31uw",
        "outputId": "e99950fb-9b41-4395-df73-3d0d5656323f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üîÑ D√©but des transformations...\n",
            "‚úÖ Transformations termin√©es.\n",
            "‚úÖ Fichier nettoy√© sauvegard√© : /content/openfood_referentiel_cleaned.csv\n",
            " 1000 lignes import√©es dans project-final-laka-93110.Laka10.openfoodfacts.\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# === √âtape 1 : Chargement du fichier CSV ===\n",
        "\n",
        "original_csv_path = \"/content/openfood_referentiel.csv\"\n",
        "cleaned_csv_path = \"/content/openfood_referentiel_cleaned.csv\"\n",
        "\n",
        "df = pd.read_csv(original_csv_path)\n",
        "\n",
        "# Nettoyage des noms de colonnes\n",
        "def clean_column_name(col):\n",
        "    col = col.strip()\n",
        "    col = re.sub(r\"[^\\w]\", \"_\", col)\n",
        "    col = re.sub(r\"__+\", \"_\", col)\n",
        "    return col.strip(\"_\")\n",
        "\n",
        "df.columns = [clean_column_name(col) for col in df.columns]\n",
        "\n",
        "# === √âtape 2 : Transformation des donn√©es ===\n",
        "\n",
        "print(\"D√©but des transformations...\")\n",
        "\n",
        "# Convertir les colonnes num√©riques\n",
        "numeric_cols = [\n",
        "    \"energy_kcal\", \"fat_100g\", \"saturated_fat_100g\",\n",
        "    \"sugars_100g\", \"salt_100g\", \"fiber_100g\", \"proteins_100g\",\n",
        "    \"nutrition_score_fr\"\n",
        "]\n",
        "for col in numeric_cols:\n",
        "    if col in df.columns:\n",
        "        df[col] = pd.to_numeric(df[col], errors=\"coerce\")\n",
        "\n",
        "# Valeurs manquantes et nettoyage de base\n",
        "df[\"product_name\"] = df[\"product_name\"].fillna(\"Inconnu\")\n",
        "df[\"nutriscore_grade\"] = df[\"nutriscore_grade\"].fillna(\"\").str.upper()\n",
        "for col in [\"labels\", \"brands\", \"categories\", \"origins\", \"stores\"]:\n",
        "    df[col] = df[col].fillna(\"\").str.lower().str.strip()\n",
        "\n",
        "# Dictionnaires de traduction\n",
        "translations = {\n",
        "    \"organic\": \"bio\",\n",
        "    \"gluten-free\": \"sans gluten\",\n",
        "    \"vegetarian\": \"v√©g√©tarien\",\n",
        "    \"vegan\": \"v√©g√©talien\",\n",
        "    \"non-gmo\": \"sans OGM\",\n",
        "    \"halal\": \"halal\",\n",
        "    \"kosher\": \"kasher\",\n",
        "    \"beverages\": \"boissons\",\n",
        "    \"dairies\": \"produits laitiers\",\n",
        "    \"sodas\": \"sodas\",\n",
        "    \"snacks\": \"snacks\",\n",
        "    \"cereals\": \"c√©r√©ales\",\n",
        "    \"meats\": \"viandes\",\n",
        "    \"ready-meals\": \"plats pr√©par√©s\",\n",
        "    \"breakfasts\": \"petits-d√©jeuners\",\n",
        "    \"cheeses\": \"fromages\",\n",
        "    \"desserts\": \"desserts\",\n",
        "    \"france\": \"France\",\n",
        "    \"germany\": \"Allemagne\",\n",
        "    \"italy\": \"Italie\",\n",
        "    \"spain\": \"Espagne\",\n",
        "    \"carrefour\": \"Carrefour\",\n",
        "    \"leclerc\": \"Leclerc\",\n",
        "    \"lidl\": \"Lidl\",\n",
        "    \"auchan\": \"Auchan\",\n",
        "    \"monoprix\": \"Monoprix\"\n",
        "}\n",
        "\n",
        "def translate_text(text):\n",
        "    for eng, fr in translations.items():\n",
        "        text = re.sub(rf\"\\b{eng}\\b\", fr, text, flags=re.IGNORECASE)\n",
        "    return text\n",
        "\n",
        "# Dictionnaire de translitt√©ration arabe ‚Üí fran√ßais\n",
        "arabic_to_french = {\n",
        "    \"ÿ≥ŸÑÿ∑ÿßŸÜ\": \"Sultan\",\n",
        "    \"ÿßŸÑÿ±ÿßŸäÿ©\": \"Al-Raya\",\n",
        "    \"ŸÉÿßÿ±ŸÅŸàÿ±\": \"Carrefour\",\n",
        "    \"ÿ£Ÿàÿ¥ÿßŸÜ\": \"Auchan\",\n",
        "    \"ŸÑŸäÿØŸÑ\": \"Lidl\"\n",
        "}\n",
        "\n",
        "def transliterate_text(text):\n",
        "    for ar, fr in arabic_to_french.items():\n",
        "        text = re.sub(rf\"\\b{ar}\\b\", fr, text)\n",
        "    return text\n",
        "\n",
        "# Appliquer traduction + translitt√©ration\n",
        "for col in [\"labels\", \"categories\", \"stores\", \"brands\", \"origins\"]:\n",
        "    df[col] = df[col].apply(translate_text)\n",
        "    df[col] = df[col].apply(transliterate_text)\n",
        "    df[col] = df[col].str.title().str.strip()\n",
        "\n",
        "# Ajout de colonnes d√©riv√©es\n",
        "df[\"has_label_bio\"] = df[\"labels\"].str.contains(\"bio\", case=False, na=False)\n",
        "\n",
        "df[\"nutrient_density\"] = df[\"energy_kcal\"] / (\n",
        "    df[\"proteins_100g\"].fillna(0) +\n",
        "    df[\"fat_100g\"].fillna(0) +\n",
        "    df[\"sugars_100g\"].fillna(0)\n",
        ").replace(0, 1)\n",
        "\n",
        "def classify_nutriscore(score):\n",
        "    score = str(score).strip().upper()\n",
        "    return {\n",
        "        \"A\": \"Excellent\",\n",
        "        \"B\": \"Bon\",\n",
        "        \"C\": \"Moyen\",\n",
        "        \"D\": \"M√©diocre\",\n",
        "        \"E\": \"Mauvais\"\n",
        "    }.get(score, \"Inconnu\")\n",
        "\n",
        "df[\"qualite_nutritionnelle\"] = df[\"nutriscore_grade\"].apply(classify_nutriscore)\n",
        "\n",
        "print(\"Transformations termin√©es.\")\n",
        "\n",
        "# === √âtape 3 : Sauvegarde du fichier transform√© ===\n",
        "\n",
        "df.to_csv(cleaned_csv_path, index=False)\n",
        "print(f\"Fichier nettoy√© sauvegard√© : {cleaned_csv_path}\")\n",
        "\n",
        "# === √âtape 4 : Chargement dans BigQuery ===\n",
        "\n",
        "os.environ[\"GOOGLE_APPLICATION_CREDENTIALS\"] = \"/content/project-final-laka-93110-e98c1369c8cc.json\"\n",
        "\n",
        "PROJECT_ID = \"project-final-laka-93110\"\n",
        "DATASET_ID = \"Laka10\"\n",
        "TABLE_ID = \"openfoodfacts\"\n",
        "BQ_TABLE = f\"{PROJECT_ID}.{DATASET_ID}.{TABLE_ID}\"\n",
        "\n",
        "client = bigquery.Client(project=PROJECT_ID)\n",
        "\n",
        "job_config = bigquery.LoadJobConfig(\n",
        "    source_format=bigquery.SourceFormat.CSV,\n",
        "    skip_leading_rows=1,\n",
        "    autodetect=True,\n",
        "    write_disposition=\"WRITE_TRUNCATE\"\n",
        ")\n",
        "\n",
        "with open(cleaned_csv_path, \"rb\") as source_file:\n",
        "    load_job = client.load_table_from_file(source_file, BQ_TABLE, job_config=job_config)\n",
        "\n",
        "load_job.result()\n",
        "\n",
        "table = client.get_table(BQ_TABLE)\n",
        "print(f\" {table.num_rows} lignes import√©es dans {BQ_TABLE}.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5jTkatckQh6V",
        "outputId": "8d26ab50-fc98-4132-b0a7-4bf56361f11c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üîÑ D√©but des transformations...\n",
            "‚úÖ Transformations termin√©es.\n",
            "‚úÖ Fichier nettoy√© sauvegard√© : /content/openfood_referentiel_cleaned.csv\n",
            "‚úÖ 613 lignes import√©es dans project-final-laka-93110.Laka10.openfoodfacts.\n"
          ]
        }
      ],
      "source": [
        "# === √âtape 1 : Chargement du fichier CSV ===\n",
        "\n",
        "original_csv_path = \"/content/openfood_referentiel.csv\"\n",
        "cleaned_csv_path = \"/content/openfood_referentiel_cleaned.csv\"\n",
        "\n",
        "df = pd.read_csv(original_csv_path)\n",
        "\n",
        "# Nettoyage des noms de colonnes\n",
        "def clean_column_name(col):\n",
        "    col = col.strip()\n",
        "    col = re.sub(r\"[^\\w]\", \"_\", col)\n",
        "    col = re.sub(r\"__+\", \"_\", col)\n",
        "    return col.strip(\"_\")\n",
        "\n",
        "df.columns = [clean_column_name(col) for col in df.columns]\n",
        "\n",
        "# === √âtape 2 : Transformation des donn√©es ===\n",
        "\n",
        "print(\"D√©but des transformations...\")\n",
        "\n",
        "# Convertir les colonnes num√©riques\n",
        "numeric_cols = [\n",
        "    \"energy_kcal\", \"fat_100g\", \"saturated_fat_100g\",\n",
        "    \"sugars_100g\", \"salt_100g\", \"fiber_100g\", \"proteins_100g\",\n",
        "    \"nutrition_score_fr\"\n",
        "]\n",
        "for col in numeric_cols:\n",
        "    if col in df.columns:\n",
        "        df[col] = pd.to_numeric(df[col], errors=\"coerce\")\n",
        "\n",
        "# Nettoyage de base\n",
        "df[\"product_name\"] = df[\"product_name\"].fillna(\"Inconnu\")\n",
        "df[\"nutriscore_grade\"] = df[\"nutriscore_grade\"].fillna(\"\").str.upper()\n",
        "for col in [\"labels\", \"brands\", \"categories\", \"origins\", \"stores\"]:\n",
        "    df[col] = df[col].fillna(\"\").str.lower().str.strip()\n",
        "\n",
        "# Suppression des lignes inutiles ou trop incompl√®tes\n",
        "df = df[df[\"product_name\"].str.lower() != \"inconnu\"]\n",
        "min_non_null = len(df) * 0.5\n",
        "df = df.dropna(thresh=min_non_null, axis=1)\n",
        "df = df.dropna(subset=[\"energy_kcal\", \"sugars_100g\", \"saturated_fat_100g\", \"proteins_100g\", \"fiber_100g\"])\n",
        "\n",
        "# Ajout de la colonne calcul√©e\n",
        "df[\"scoring_nutritionnel_personnalise\"] = (\n",
        "    df[\"energy_kcal\"] + df[\"sugars_100g\"] + df[\"saturated_fat_100g\"] -\n",
        "    (df[\"proteins_100g\"] + df[\"fiber_100g\"])\n",
        ")\n",
        "\n",
        "# Traductions\n",
        "translations = {\n",
        "    \"organic\": \"bio\",\n",
        "    \"gluten-free\": \"sans gluten\",\n",
        "    \"vegetarian\": \"v√©g√©tarien\",\n",
        "    \"vegan\": \"v√©g√©talien\",\n",
        "    \"non-gmo\": \"sans OGM\",\n",
        "    \"halal\": \"halal\",\n",
        "    \"kosher\": \"kasher\",\n",
        "    \"beverages\": \"boissons\",\n",
        "    \"dairies\": \"produits laitiers\",\n",
        "    \"sodas\": \"sodas\",\n",
        "    \"snacks\": \"snacks\",\n",
        "    \"cereals\": \"c√©r√©ales\",\n",
        "    \"meats\": \"viandes\",\n",
        "    \"ready-meals\": \"plats pr√©par√©s\",\n",
        "    \"breakfasts\": \"petits-d√©jeuners\",\n",
        "    \"cheeses\": \"fromages\",\n",
        "    \"desserts\": \"desserts\",\n",
        "    \"france\": \"France\",\n",
        "    \"germany\": \"Allemagne\",\n",
        "    \"italy\": \"Italie\",\n",
        "    \"spain\": \"Espagne\",\n",
        "    \"carrefour\": \"Carrefour\",\n",
        "    \"leclerc\": \"Leclerc\",\n",
        "    \"lidl\": \"Lidl\",\n",
        "    \"auchan\": \"Auchan\",\n",
        "    \"monoprix\": \"Monoprix\"\n",
        "}\n",
        "\n",
        "def translate_text(text):\n",
        "    for eng, fr in translations.items():\n",
        "        text = re.sub(rf\"\\b{eng}\\b\", fr, text, flags=re.IGNORECASE)\n",
        "    return text\n",
        "\n",
        "arabic_to_french = {\n",
        "    \"ÿ≥ŸÑÿ∑ÿßŸÜ\": \"Sultan\",\n",
        "    \"ÿßŸÑÿ±ÿßŸäÿ©\": \"Al-Raya\",\n",
        "    \"ŸÉÿßÿ±ŸÅŸàÿ±\": \"Carrefour\",\n",
        "    \"ÿ£Ÿàÿ¥ÿßŸÜ\": \"Auchan\",\n",
        "    \"ŸÑŸäÿØŸÑ\": \"Lidl\"\n",
        "}\n",
        "\n",
        "def transliterate_text(text):\n",
        "    for ar, fr in arabic_to_french.items():\n",
        "        text = re.sub(rf\"\\b{ar}\\b\", fr, text)\n",
        "    return text\n",
        "\n",
        "for col in [\"labels\", \"categories\", \"stores\", \"brands\", \"origins\"]:\n",
        "    df[col] = df[col].apply(translate_text)\n",
        "    df[col] = df[col].apply(transliterate_text)\n",
        "    df[col] = df[col].str.title().str.strip()\n",
        "\n",
        "# Ajouter une d√©tection du label \"bio\"\n",
        "df[\"has_label_bio\"] = df[\"labels\"].str.contains(\"bio\", case=False, na=False)\n",
        "\n",
        "# Densit√© nutritionnelle\n",
        "df[\"nutrient_density\"] = df[\"energy_kcal\"] / (\n",
        "    df[\"proteins_100g\"].fillna(0) +\n",
        "    df[\"fat_100g\"].fillna(0) +\n",
        "    df[\"sugars_100g\"].fillna(0)\n",
        ").replace(0, 1)\n",
        "\n",
        "# Classification du nutriscore\n",
        "def classify_nutriscore(score):\n",
        "    score = str(score).strip().upper()\n",
        "    return {\n",
        "        \"A\": \"Excellent\",\n",
        "        \"B\": \"Bon\",\n",
        "        \"C\": \"Moyen\",\n",
        "        \"D\": \"M√©diocre\",\n",
        "        \"E\": \"Mauvais\"\n",
        "    }.get(score, \"Inconnu\")\n",
        "\n",
        "df[\"qualite_nutritionnelle\"] = df[\"nutriscore_grade\"].apply(classify_nutriscore)\n",
        "\n",
        "# === (Optionnel) Traduire les noms de produits localement (commentaire) ===\n",
        "# from deep_translator import GoogleTranslator\n",
        "# df[\"product_name\"] = df[\"product_name\"].apply(lambda x: GoogleTranslator(source='auto', target='fr').translate(x))\n",
        "\n",
        "print(\"Transformations termin√©es.\")\n",
        "\n",
        "# === √âtape 3 : Sauvegarde ===\n",
        "\n",
        "df.to_csv(cleaned_csv_path, index=False)\n",
        "print(f\"Fichier nettoy√© sauvegard√© : {cleaned_csv_path}\")\n",
        "\n",
        "# === √âtape 4 : Chargement dans BigQuery ===\n",
        "\n",
        "os.environ[\"GOOGLE_APPLICATION_CREDENTIALS\"] = \"/content/project-final-laka-93110-e98c1369c8cc.json\"\n",
        "\n",
        "PROJECT_ID = \"project-final-laka-93110\"\n",
        "DATASET_ID = \"Laka10\"\n",
        "TABLE_ID = \"openfoodfacts\"\n",
        "BQ_TABLE = f\"{PROJECT_ID}.{DATASET_ID}.{TABLE_ID}\"\n",
        "\n",
        "client = bigquery.Client(project=PROJECT_ID)\n",
        "\n",
        "job_config = bigquery.LoadJobConfig(\n",
        "    source_format=bigquery.SourceFormat.CSV,\n",
        "    skip_leading_rows=1,\n",
        "    autodetect=True,\n",
        "    write_disposition=\"WRITE_TRUNCATE\"\n",
        ")\n",
        "\n",
        "with open(cleaned_csv_path, \"rb\") as source_file:\n",
        "    load_job = client.load_table_from_file(source_file, BQ_TABLE, job_config=job_config)\n",
        "\n",
        "load_job.result()\n",
        "table = client.get_table(BQ_TABLE)\n",
        "print(f\"{table.num_rows} lignes import√©es dans {BQ_TABLE}.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MatWMOs2IFnA"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JRN2tlS7IFpU"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kDHs0C5kIFuG"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LRIWOBYtIFv0"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
